{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3NaiSGpEJUU2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_aQ2gp4KOHR",
        "outputId": "2e8a731f-34e5-4185-aba5-3d07596d387c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4qEveauKUrZ",
        "outputId": "40c1b6d9-4d46-486d-b980-83b28193f8e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCRnbT_lK9lS",
        "outputId": "a8a4e7aa-bd77-4b78-e5d0-9c123bfdd5e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4EgeBaKLB0B",
        "outputId": "7919f325-5f51-499a-94bb-99585a78a216"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the text\n",
        "### Vectorize the text"
      ],
      "metadata": {
        "id": "pui2gUkMLMmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGAPuCwTLKDK",
        "outputId": "d4e611ea-29dc-442c-a32d-0b51bd82efd0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "ct0IDZEoLeGK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBkoYkpgLomi",
        "outputId": "eccf772e-d716-49d1-ad5e-d20c19dcf1f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None\n",
        ")"
      ],
      "metadata": {
        "id": "qjrLD5L_LqyB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ids_from_chars.get_vocabulary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwaliDQTL2ES",
        "outputId": "61cb37a7-c614-4f3b-d72c-1dcae196a586"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DH_nWc8MA2K",
        "outputId": "fef517d1-e1a9-431b-ca73-b70e3061b107"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXrlhg-pMITh",
        "outputId": "2fd45b43-3266-4db2-fb86-1e1cc140c24a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "zOB14ZAVMVoi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training examples and targets"
      ],
      "metadata": {
        "id": "n2WDPWVeNNDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FvwhfAOMf3Z",
        "outputId": "67c13729-63b6-4f71-ded8-d66c1e5b3b25"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "rKA798zFNXkI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "  print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY9uMLxYNd-B",
        "outputId": "e1158779-6ac7-49a3-d966-ce6d3b2bcf94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "8fI94am5Nkt5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_xnUlsqNpTJ",
        "outputId": "6311694d-05cb-4ea1-e4d0-0a2e83a1e629"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0yHcHifNyw-",
        "outputId": "91b0e2cf-0b11-4b64-e6f6-e2219cccc784"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "6p6j5bC2N6dS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8vxSXIYOO9q",
        "outputId": "7fbbe711-d15d-4c38-8761-c59188a233b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "twtmq64ZOR0S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target :\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHMLPtb6OXfw",
        "outputId": "86711da5-c8a8-41b8-b054-dac5c1339c75"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target : b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create training batches"
      ],
      "metadata": {
        "id": "z_0i2SwpOofX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
        ")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ehpcda1OiUR",
        "outputId": "5488079d-b6de-4e99-b170-4f11681c8b19"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build The Model"
      ],
      "metadata": {
        "id": "IWOQzWdZQChF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "j6XADW--QACZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "bhE59fWYQnFB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        "    )"
      ],
      "metadata": {
        "id": "rSCEkYQDRpOR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try the model"
      ],
      "metadata": {
        "id": "sN9wHpLolIMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)"
      ],
      "metadata": {
        "id": "fVNqe0JTRyfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a4181e-305d-4018-f531-3d3bfd804e46"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKRBX6zglSTO",
        "outputId": "3b525e70-7d17-4656-cddb-d6868eb52042"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "-8UA3WF_ly0E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-mMvmVfl7Jz",
        "outputId": "1f1c1b9c-467c-47c6-d6ea-a0454076240b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([60, 43, 56, 61, 23,  8, 38, 49, 31, 55, 11, 51, 31, 22, 31, 35, 21,\n",
              "       28,  7,  4, 64, 15,  8, 10, 48, 42,  3, 55, 13, 39, 60,  4,  7, 59,\n",
              "       38, 54, 59, 15, 11, 42, 33, 38, 13, 21, 34, 50, 19, 40, 47, 48, 15,\n",
              "       62, 41, 49, 15, 31, 58, 48, 62, 64, 35, 63,  7,  8, 34, 23, 47, 23,\n",
              "        3, 36, 16, 19, 32, 21, 51, 59, 54, 15,  2,  6, 59, 12, 21, 26, 33,\n",
              "       29, 36, 37, 63, 41, 12, 25, 62, 33, 52, 18, 28, 62, 45, 34])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqEgYlf6mOSt",
        "outputId": "2a4d173d-b607-40f5-c1c6-0617efdc762f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'mberland.\\n\\nNORTHUMBERLAND:\\nThe commons will not then be satisfied.\\n\\nKING RICHARD II:\\nThey shall be s'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"udqvJ-YjRp:lRIRVHO,$yB-3ic!p?Zu$,tYotB:cTY?HUkFahiBwbjBRsiwyVx,-UJhJ!WCFSHltoB 't;HMTPWXxb;LwTmEOwfU\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "XIBf4r0OmpJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "zSndpn6ZmbYs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epSQSiV0myWs",
        "outputId": "afc820cf-3a1e-45b3-ca15-0a0c2a31dfb2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.189123, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1vx_YbUm6Hj",
        "outputId": "17f7ddf9-d361-4373-bef2-aeb071596160"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.96493"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=loss\n",
        ")"
      ],
      "metadata": {
        "id": "lQxhBPNInKXC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure checkpoints"
      ],
      "metadata": {
        "id": "EcyXhDXRndcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "rpxnYmqinU9c"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute the training"
      ],
      "metadata": {
        "id": "xb-EK5b6nvpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "7xSS39-4ntvk"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38asLX9wnx0q",
        "outputId": "287da543-5d5e-451e-d1c6-eb76ca277e9a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 10s 50ms/step - loss: 0.4689\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 50ms/step - loss: 0.4625\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 0.4567\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.4521\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 0.4448\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 0.4438\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 0.4426\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 0.4347\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 10s 52ms/step - loss: 0.4320\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 13s 55ms/step - loss: 0.4274\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 0.4273\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 13s 54ms/step - loss: 0.4233\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.4280\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.4319\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.4290\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.4265\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 0.4376\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 0.4300\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 13s 56ms/step - loss: 0.4285\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.4300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate text"
      ],
      "metadata": {
        "id": "1VY2KTrqoBGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index\n",
        "        values = [-float('inf')] * len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape = [len(ids_from_chars.get_vocabulary())]\n",
        "    )\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "    # Only use the last prediction\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "tOXEOzwZn-te"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(\n",
        "    model,\n",
        "    chars_from_ids,\n",
        "    ids_from_chars\n",
        ")"
      ],
      "metadata": {
        "id": "KCxidaH3rN5L"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oWoquf3sdEO",
        "outputId": "d59b8214-908b-4f7a-ebf6-ec54e5826b0c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The early more deny:\n",
            "How have you news of our here prince, be gone!\n",
            "But yet my land I swear thy name, but that I do.\n",
            "\n",
            "MERCUTIO:\n",
            "Any, to appear, and now 'tis this day say: what\n",
            "A joy it like your good friendships may beseem him come.\n",
            "\n",
            "ONIO:\n",
            "Haply come; fellow Madam:\n",
            "That is no bitter'd traitors? but the strong\n",
            "Their means to-morrow mocks, or royal father.\n",
            "\n",
            "GLOUCESTER:\n",
            "By heaven, I will not die before me.\n",
            "Well, I do beseech your meaning, brief would think.\n",
            "\n",
            "KATHARINA:\n",
            "Go you to him for joyful majesty!\n",
            "Jacket them not; had shames her death,\n",
            "And every thing you were used to say.\n",
            "\n",
            "WABELLA:\n",
            "My bud impossiby lace deform.\n",
            "\n",
            "PROSPERO:\n",
            "Come, fearful how the porce.\n",
            "But, O, my lords, when I far brought to high again\n",
            "Is counted vasting fair,--not breath;\n",
            "For being an advocate the househorts strueg at him,\n",
            "And given away twell out of doors\n",
            "By nighting thus?\n",
            "How now, no more.\n",
            "\n",
            "COMINIUS:\n",
            "I know you look not what; while we put themselves\n",
            "Like palm clush'd all that high acoided upright\n",
            "As you were told  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.5752103328704834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ],
      "metadata": {
        "id": "4LjU7rEXt7aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK-18oLas_-Z",
        "outputId": "de6a2cbc-7370-4ff1-d03d-8aa1f16c41a4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe wisdom night.\\nGo they; and littly counsel to your deliverance\\nAs far for fellow true full as your confess'd,\\nAnd every honour is a tunkled friends.\\n\\nMARCIUS:\\nHow now, who hath deserved with trouble stain\\nWhich he dread cloak I have no more.'\\n\\nAEdile:\\nWise! this is no more; adding my heart\\nMay well be satisfray; but yet, sir!\\nShe is yourself.\\n\\nROMEO:\\nNot so well as I halt by the squire;\\nAnd see her hand him like a doman of a\\nmistress of this all men light his hate.\\n\\nGLOUCESTER:\\nWill not you good?\\n\\nSecond Servant:\\nI do beseech your grace to pardon me:\\nMy lord and you not yours for him. Come,\\nSignior Lucentio, whom here he loves me,\\nAnd gash behind in due by me. What ease it resides?\\n\\nANGELO:\\nHow doth my daughter?\\n\\nFirst Servant:\\nA widow of a woman's war,\\nThat Rogach, tears an ancient strike!\\n\\nBAPTISTA:\\nYou have mind: else it so: but not I dead.\\nHow art thou fickle: my lords, he did it us\\nAnd welcome. Broand, cousin; but one word more;\\nFor I will have them known by 't.\\n\\nGONZALO:\\nTell\"\n",
            " b\"ROMEO:\\nThe early servect thee! dare no little;\\nThe one my wind and hers, he will take here.\\n\\nSLY:\\nAnd his which nature murders my heart!\\n'Tis nothing.\\n\\nCAMILLO:\\nNo remission of it.\\n\\nBUCKINGHAM:\\nBut my, lords have I left me to the right Vince\\nAn easy day for this! O more town in me;\\nAnd so defect.\\n\\nLUCIO:\\nHow now, my oathwithin thou hast now eleven you\\nhave not a good father. Let it go,\\nWe met him in your brothers brothers,\\nAnd with such libbrain'd recloring wars in safety\\nHer water on the house of York such head\\nAs what o'clock the Strangeners.\\nAnd kiss her son. Aumerle is not\\nso great, sir. This day shall see, when you are\\nwell been long, but I will not knock my hands,\\nNor witt thy fortune, by the hair, that\\nRomeow deny is thy less expedly\\nAnd quath-hard his delivered while we have learn'd,\\nThat we may palm, which take crept on thee,\\nBut to rejoice in melancholy bed;\\nThis scandal's love, I here protest, had sound\\nTurns it o' the love have by my pammers;\\nAnd beg's milliken mourning to the ti\"\n",
            " b\"ROMEO:\\nSpoke like an agged by your beholding, once to God!\\nhere! so that it shall be conducted\\nYour condemned so die, as it were, friends! Even it will before might\\nwith cowardly of you should frailty.\\n\\nCLIFFORD:\\nHad he beheld to fetch a destroy of thine;\\nFor which my youngest daughter, proud me of;\\nAnd many an whilst thou hast slept with thee:\\nFall lik us nothing in duligy sound.\\n\\nEDWARD:\\nNow you are one in its so;\\nFor, were it not, sir; do you two know?\\n\\nQOENZELIA:\\nNo, my good Lord of Beckolat,\\n'Tis thought that my report is he offend.\\nI have a toil my mother's hand.\\nArise, be gone: ballain, not the purpose.\\nWhat muffled of all this? spoke of thy nose;\\nThe common army apparel, telling him of her.\\n\\nROMEO:\\nO, to Thy father like the absent doubt,\\nI never sue to seek the dead of you;\\nFor Givor and Edward, hath might have been dead,\\nSo well king her hence and be gone a pent\\nCompare Elit, to my sin with what haply humbly\\nWill have no more stretch this dead king.\\n\\nWARWICK:\\nWhy, then, my lord.\\n\\nHE\"\n",
            " b\"ROMEO:\\nThe exactather on the market-place! My unfignified\\nThat heaven fertlemanly, than they are not\\nChrong against thy wilful arms.\\n\\nDUKE VINCENTIO:\\nThere is no note of mine dogriss, you must swear\\nThe shallest wit of mine own tongue,\\nfor no warrant.\\n\\nSICINIUS:\\nSee the common earth the health!\\nWhat, no in else of happy days so name?\\n\\nVOLUMNIA:\\nAnger's my gentleman conspiraced, but keep so fair a cast!\\nI hold rich my bold expection to slander,\\nWith cockation than Adwar'd his sickness's words\\nAnd my slave's receips with death is grown\\nby the war; between whisper the instant cass.\\nIs this so favours while the rumble three shepherds:\\nThese holy summer from their brows:\\nHath an executing the gall;\\nAnd when the king by this dead earling?\\n\\nJULIET:\\nMadam, uncle, that thy valour! Whose uncles\\nTo have the long this isle, the sweets of bit\\nrevolts or charges and treacherous, and from such a beggar,\\nWhose hell measuries to most calliams are\\ndenigabled a peasant chebul after:\\nAre you are well, Clarence,\"\n",
            " b\"ROMEO:\\nThe wisdom Lord Hundsman Tybalt's day,\\nAnd twite were noiful to the end o' the world.\\n\\nFirst Senator:\\nThen.\\n\\nCORIOLANUS:\\nSufford resent lie, my sovereign\\nLike a monstron; therefore play'd the regions\\nOf an old babe, and learn this blessed husband\\nmeasure the loss of his outrage,--Od Clarence take,\\nAnd thou shalt live to see the mustard of your place,\\nAnd confess to all that shall win my name\\nI have in dangerous trigual drink.\\n\\nKING HENRY VI:\\nBut, with thy father, never was before:\\nHad year mine, or the land I cannot never\\nBe held a brace.\\nBENVOLUSEO:\\nBriet to Hear not you will not stay these wonderful.\\n\\nKING HENRY VI:\\nAnd whither say I have no more to sap\\nThe shadow or your brother.\\n\\nISABELLA:\\nLet me have way; but you are\\nit to thyself and my young Romeo,\\nIn that three o'clock, sweet brother; Ay\\nShall be the chair of state; alas! slave, where'er I slew in favour\\nhis hum like a loving tiding; no, above me.\\nRevont of his sister. But what have we have to take\\nSlew him to prishel.\\n\\nAUTOLY\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.015928030014038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXV69EmMt9JT",
        "outputId": "ffa411a9-f134-4078-ce8e-aec96ae8e165"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7cfa6867a2c0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkLOljqwuLu7",
        "outputId": "ab0cc539-e9b6-429e-931a-7b9286c59aee"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The exchange of thy childish fool,\n",
            "I am sorry, one could, our fearful man\n",
            "He gave this churchyary r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced: Customized Training"
      ],
      "metadata": {
        "id": "b4oK9s4huWEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = self(inputs, training=True)\n",
        "        loss = self.loss(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "id": "1_W47tvguPIb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units\n",
        "  )"
      ],
      "metadata": {
        "id": "1ZpK8USHuq9j"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  )"
      ],
      "metadata": {
        "id": "GuahxKbLuuzM"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-1whywNux_c",
        "outputId": "b674764f-097d-4d16-c3f5-267c9c9460d3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 14s 50ms/step - loss: 2.6953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cfa616a1030>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or if we need more control, we can write our own complete custom training loop:"
      ],
      "metadata": {
        "id": "hbZfsGlrvFjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "      logs = model.train_step([inp, target])\n",
        "      mean.update_state(logs['loss'])\n",
        "\n",
        "      if batch_n % 50 == 0:\n",
        "          template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "          print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwt7bwh1u5fb",
        "outputId": "c70f3e28-2766-4e3b-abaf-ee569ae26171"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1674\n",
            "Epoch 1 Batch 50 Loss 2.0711\n",
            "Epoch 1 Batch 100 Loss 1.9359\n",
            "Epoch 1 Batch 150 Loss 1.8572\n",
            "\n",
            "Epoch 1 Loss: 1.9716\n",
            "Time taken for 1 epoch 14.02 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7956\n",
            "Epoch 2 Batch 50 Loss 1.6928\n",
            "Epoch 2 Batch 100 Loss 1.6914\n",
            "Epoch 2 Batch 150 Loss 1.5794\n",
            "\n",
            "Epoch 2 Loss: 1.6973\n",
            "Time taken for 1 epoch 10.23 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6021\n",
            "Epoch 3 Batch 50 Loss 1.6051\n",
            "Epoch 3 Batch 100 Loss 1.5584\n",
            "Epoch 3 Batch 150 Loss 1.5106\n",
            "\n",
            "Epoch 3 Loss: 1.5384\n",
            "Time taken for 1 epoch 10.75 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4185\n",
            "Epoch 4 Batch 50 Loss 1.4522\n",
            "Epoch 4 Batch 100 Loss 1.4561\n",
            "Epoch 4 Batch 150 Loss 1.4278\n",
            "\n",
            "Epoch 4 Loss: 1.4420\n",
            "Time taken for 1 epoch 10.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3980\n",
            "Epoch 5 Batch 50 Loss 1.3709\n",
            "Epoch 5 Batch 100 Loss 1.3518\n",
            "Epoch 5 Batch 150 Loss 1.3645\n",
            "\n",
            "Epoch 5 Loss: 1.3751\n",
            "Time taken for 1 epoch 10.20 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3327\n",
            "Epoch 6 Batch 50 Loss 1.3157\n",
            "Epoch 6 Batch 100 Loss 1.3170\n",
            "Epoch 6 Batch 150 Loss 1.3113\n",
            "\n",
            "Epoch 6 Loss: 1.3217\n",
            "Time taken for 1 epoch 10.22 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2509\n",
            "Epoch 7 Batch 50 Loss 1.2863\n",
            "Epoch 7 Batch 100 Loss 1.2718\n",
            "Epoch 7 Batch 150 Loss 1.2959\n",
            "\n",
            "Epoch 7 Loss: 1.2777\n",
            "Time taken for 1 epoch 10.19 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2528\n",
            "Epoch 8 Batch 50 Loss 1.2377\n",
            "Epoch 8 Batch 100 Loss 1.2514\n",
            "Epoch 8 Batch 150 Loss 1.2608\n",
            "\n",
            "Epoch 8 Loss: 1.2371\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1952\n",
            "Epoch 9 Batch 50 Loss 1.1963\n",
            "Epoch 9 Batch 100 Loss 1.2069\n",
            "Epoch 9 Batch 150 Loss 1.1862\n",
            "\n",
            "Epoch 9 Loss: 1.1968\n",
            "Time taken for 1 epoch 10.78 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1276\n",
            "Epoch 10 Batch 50 Loss 1.1510\n",
            "Epoch 10 Batch 100 Loss 1.1408\n",
            "Epoch 10 Batch 150 Loss 1.1645\n",
            "\n",
            "Epoch 10 Loss: 1.1565\n",
            "Time taken for 1 epoch 11.39 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "VjorILJqvH1U"
      }
    }
  ]
}